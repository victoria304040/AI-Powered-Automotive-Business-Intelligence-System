import os
import pandas as pd
import glob
from typing import List, Dict, Any, Optional
from langchain.agents.agent_types import AgentType
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.tools import tool
from langchain.tools.render import format_tool_to_openai_function

print("ç•¶å‰å·¥ä½œç›®éŒ„ï¼š", os.getcwd())
print("è©²ç›®éŒ„ä¸‹çš„ Excel æª”æ¡ˆåˆ—è¡¨ï¼š", glob.glob("*.xlsx"))


# ==================================== 1. è¨­å®š ====================================
# ç¢ºä¿ API é‡‘é‘°å·²è¨­å®š
if not os.environ.get("OPENAI_API_KEY"):
    with open("secret_key", "r", encoding="utf-8") as f:
        os.environ["OPENAI_API_KEY"] = f.read().strip()

llm = ChatOpenAI(temperature=0, model="gpt-4o-2024-11-20")

# å…¨åŸŸå­—å…¸å„²å­˜å¤šå€‹ DataFrame
dataframes = {}


# ==================================== 2. å®šç¾©è‡ªè¨‚å·¥å…·å‡½æ•¸ ====================================
@tool
def list_and_classify_files(file_extension: str = "xlsx") -> Dict[str, List[str]]:
    """
    å›å‚³åˆ†é¡å¾Œçš„æª”æ¡ˆåˆ—è¡¨ï¼štarget / actual / unknown
    """
    files = glob.glob(f"*.{file_extension}")
    grouped = {"target": [], "actual": [], "unknown": []}

    for f in files:
        classification = classify_file_type.invoke({"filename": f}).get("classification", "unknown")
        grouped[classification].append(f)

    return grouped

@tool
def load_excel_file(filename: str, preview_rows: int = 5) -> Dict:
    """
    è¼‰å…¥ Excel æ‰€æœ‰å·¥ä½œè¡¨ï¼Œè³‡æ–™å„²å­˜æ–¼å…¨åŸŸè®Šæ•¸ dataframesï¼Œkey ç‚º filename::sheetã€‚
    å›å‚³æ¯å€‹å·¥ä½œè¡¨çš„æ¬„ä½èˆ‡å‰å¹¾åˆ—é è¦½ã€‚
    """
    global dataframes
    try:
        xls = pd.ExcelFile(filename)
        preview = {}

        for sheet in xls.sheet_names:
            df = pd.read_excel(xls, sheet_name=sheet)
            # 1. å»é™¤æ‰€æœ‰ object æ¬„ä½çš„å‰å¾Œç©ºç™½
            df = df.apply(lambda col: col.str.strip() if col.dtype == "object" else col)

            # 2. å¼·åˆ¶è½‰æ›ã€Œæ—¥æœŸã€æ¬„ä½
            if "æ—¥æœŸ" in df.columns:
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")

            # 3. å¼·åˆ¶è½‰æ›ã€Œå¯¦ç¸¾ç¨®é¡ã€æ¬„ä½ç‚ºç´”å­—ä¸²ä¸¦ strip
            if "å¯¦ç¸¾ç¨®é¡" in df.columns:
                df["å¯¦ç¸¾ç¨®é¡"] = df["å¯¦ç¸¾ç¨®é¡"].astype(str).str.strip()

            key = f"{filename}::{sheet}"
            dataframes[key] = df
            preview[key] = {
                "columns": df.columns.tolist(),
                "sample_data": df.head(preview_rows).to_dict(orient="records")
            }

        return {
            "filename": filename,
            "sheets_loaded": len(xls.sheet_names),
            "preview": preview
        }

    except Exception as e:
        return {"error": str(e)}

@tool
def classify_file_type(filename: str) -> Dict:
    """
      åˆ†é¡è³‡æ–™è¡¨ç‚º target / actualï¼Œæ ¹æ“šæª”åèˆ‡æ¬„ä½å…§å®¹å›å‚³è©³ç´°èªªæ˜ã€‚
      """
    target_keywords = ["ç›®æ¨™", "target"]
    actual_keywords = ["çµ±è¨ˆ", "å¯¦éš›", "actual", "å¯¦ç¸¾"]

    try:
        xls = pd.ExcelFile(filename)
    except Exception as e:
        return {"filename": filename, "error": str(e)}

    for sheet in xls.sheet_names:
        try:
            df = pd.read_excel(xls, sheet_name=sheet, nrows=5)
            columns = set(df.columns.str.lower())

            if any(kw in filename.lower() or kw in sheet.lower() for kw in target_keywords):
                if columns & {"ç›®æ¨™", "target", "éŠ·å”®ç›®æ¨™", "ç¶“éŠ·å•†"}:
                    return {
                        "filename": filename,
                        "classification": "target",
                        "reason": f"æ–¼ sheetã€{sheet}ã€‘ç™¼ç¾ç›®æ¨™ç›¸é—œæ¬„ä½: {columns & {'ç›®æ¨™', 'target', 'éŠ·å”®ç›®æ¨™', 'ç¶“éŠ·å•†'}}"
                    }

            if any(kw in filename.lower() or kw in sheet.lower() for kw in actual_keywords):
                if columns & {"å¯¦éš›", "actual", "éŠ·å”®", "éŠ·å”®æ•¸", "å¯¦ç¸¾"}:
                    return {
                        "filename": filename,
                        "classification": "actual",
                        "reason": f"æ–¼ sheetã€{sheet}ã€‘ç™¼ç¾å¯¦éš›ç›¸é—œæ¬„ä½: {columns & {'å¯¦éš›', 'actual', 'éŠ·å”®', 'éŠ·å”®æ•¸'}}"
                    }

        except Exception:
            continue

    return {
        "filename": filename,
        "classification": "unknown",
        "reason": "ç„¡æ³•æ ¹æ“šæª”æ¡ˆå…§å®¹åˆ¤æ–·é¡å‹"
    }

@tool
def compare_target_vs_actual(target_key: str, actual_key: str) -> Dict[str, Any]:
    """
    æ¯”å°ç›®æ¨™èˆ‡å¯¦éš›è³‡æ–™ï¼Œåªç”¨ç¶“éŠ·å•†ä»£ç¢¼ + æ“šé»ä»£ç¢¼åš joinï¼Œ
    è‹¥å¯¦ç¸¾è¡¨ä¸­æœ‰åç¨±æ¬„ä½ï¼ˆå¦‚ ç¶“éŠ·å•†åç¨±ã€æ“šé»ï¼‰ï¼Œå‰‡åœ¨åˆä½µå¾Œä¸€ä½µå¸¶å‡ºã€‚
    key ç‚º filename::sheet_name æ ¼å¼ã€‚
    """
    import pandas as pd

    # 1. æª¢æŸ¥æ˜¯å¦å·²è¼‰å…¥
    if target_key not in dataframes or actual_key not in dataframes:
        return {"error": f"è«‹ç¢ºèªé€™å…©å€‹ key æ˜¯å¦å­˜åœ¨æ–¼ dataframesï¼š{target_key}, {actual_key}"}

    df_target = dataframes[target_key].copy()
    df_actual = dataframes[actual_key].copy()

    # 2. æ ¸å¿ƒæ¬„ä½
    dist_code_col    = "ç¶“éŠ·å•†ä»£ç¢¼"
    target_point_col = "æ“šé»ä»£ç¢¼"
    actual_point_col = "ç‡Ÿæ¥­æ‰€ä»£ç¢¼"

    # 3. ç¢ºèªå¿…è¦æ¬„ä½
    for col in (dist_code_col, target_point_col):
        if col not in df_target.columns:
            return {"error": f"ç›®æ¨™è¡¨ç¼ºå°‘å¿…è¦æ¬„ä½: {col}"}
    for col in (dist_code_col, actual_point_col):
        if col not in df_actual.columns:
            return {"error": f"å¯¦éš›è¡¨ç¼ºå°‘å¿…è¦æ¬„ä½: {col}"}

    # 4. æ‰¾éŠ·å”®æ¬„ä½
    target_sales_col = next((c for c in df_target.columns if any(k in c for k in ["ç›®æ¨™", "ç›®æ¨™æ•¸", "ç›®æ¨™éŠ·å”®æ•¸"])), None)
    actual_sales_col = next((c for c in df_actual.columns if any(k in c for k in ["å¯¦ç¸¾", "éŠ·å”®", "å—è¨‚"])), None)
    if not target_sales_col or not actual_sales_col:
        return {"error": "ç¼ºå°‘ç›®æ¨™æˆ–å¯¦éš›éŠ·å”®æ¬„ä½"}

    df_target[target_sales_col] = pd.to_numeric(df_target[target_sales_col], errors="coerce")
    df_actual[actual_sales_col] = pd.to_numeric(df_actual[actual_sales_col], errors="coerce")


    # 5. group by åªç”¨ä»£ç¢¼å»èšåˆ
    df_t = (
        df_target
        .groupby([dist_code_col, target_point_col], as_index=False)[target_sales_col]
        .sum()
        .rename(columns={target_point_col: actual_point_col, target_sales_col: "target_sales"})
    )

    # å¦‚æœå¯¦ç¸¾è¡¨æœ‰ç¶“éŠ·å•†åç¨±ã€æ“šé»åç¨±ï¼Œå°±åœ¨èšåˆæ™‚ä¸€èµ·ä¿ç•™
    extra_cols = []
    if "ç¶“éŠ·å•†åç¨±" in df_actual.columns:
        extra_cols.append("ç¶“éŠ·å•†åç¨±")
    if "æ“šé»" in df_actual.columns:
        extra_cols.append("æ“šé»")

    df_a = (
        df_actual
        .groupby([dist_code_col, actual_point_col] + extra_cols, as_index=False)[actual_sales_col]
        .sum()
        .rename(columns={actual_sales_col: "actual_sales"})
    )

    # 6. åˆä½µ
    df_merge = pd.merge(
        df_t, df_a,
        on=[dist_code_col, actual_point_col],
        how="inner"
    )
    df_merge["é”æ¨™"] = df_merge["actual_sales"] >= df_merge["target_sales"]

    # 7. å¯«å›å…¨åŸŸ
    merged_key = f"{target_key}_vs_{actual_key}"
    dataframes[merged_key] = df_merge

    # 8. summary
    total    = int(len(df_merge))
    achieved = int(df_merge["é”æ¨™"].sum())
    rate     = achieved / total if total else 0.0

    return {
        "merged_key": merged_key,
        "summary": {
            "total_matches": total,
            "achieved": achieved,
            "achievement_rate": rate
        },
        "detail": df_merge.to_dict(orient="records")
    }



# å·¥å…·é›†åˆ
tools = [list_and_classify_files, load_excel_file, classify_file_type, compare_target_vs_actual]

# ==================================== 3. è™•ç†æ˜ å°„è¡¨ï¼šå»ºç«‹ Mapping è™•ç†å‡½æ•¸ ====================================
def generate_mapping_text(mapping_path: str) -> str:
    df = pd.read_excel(mapping_path, dtype=str)

    mapping_lines = []
    for _, row in df.iterrows():
        # å»é™¤ç©ºç™½èˆ‡æ¨™æº–åŒ–
        dealer_name = row["ç¶“éŠ·å•†åç¨±"].strip()
        dealer_code = row["ç¶“éŠ·å•†ä»£ç¢¼"].strip()
        site_name = row["ç‡Ÿæ¥­æ‰€åç¨±"].strip()
        site_code = row["ç‡Ÿæ¥­æ‰€ä»£ç¢¼"].strip()

        line = f"({dealer_name}, {site_name}) â†’ ({dealer_code}, {site_code})"
        mapping_lines.append(line)

    mapping_str = "\n".join(mapping_lines)
    return f"æ˜ å°„è³‡æ–™å¦‚ä¸‹ï¼š\n{mapping_str}"

# è®€å–ä¸¦è½‰æ› mapping è³‡æ–™
mapping_text = generate_mapping_text("Mapping Dataframe.xlsx")

# ==================================== 4. å»ºç«‹ç³»çµ±è¨Šæ¯ ====================================
system_message = f"""
ä½ æ˜¯è³‡æ–™åˆ†æåŠ©ç†ï¼Œæœƒè™•ç†å…©é¡ Excel æª”æ¡ˆï¼šç›®æ¨™æª”æ¡ˆï¼ˆtargetï¼‰å’Œå¯¦éš›æª”æ¡ˆï¼ˆactualï¼‰ã€‚
æ¯å€‹æª”æ¡ˆæœƒé€é classify_file_type å·¥å…·æ¨™ç¤ºé¡å‹ï¼Œå›å‚³ JSON æ ¼å¼åŒ…å« filenameã€classificationï¼ˆ"target"ã€"actual"ã€"unknown"ï¼‰ã€reasonã€‚
è«‹æ³¨æ„ä»¥ä¸‹æ¬„ä½åç¨±åœ¨ä¸åŒè³‡æ–™è¡¨ä¸­å…·æœ‰ç›¸åŒæ„ç¾©ï¼š
- ã€Œæ“šé»ã€â‰ˆã€Œç‡Ÿæ¥­æ‰€ã€

è«‹æ ¹æ“šä»¥ä¸‹è¦å‰‡é…å°æª”æ¡ˆï¼š
- å…ˆæ¯”å°æª”åç¶“éŠ·å•†ã€ç¶“éŠ·å•†ä»£ç¢¼ç­‰è³‡è¨Š
- åŒä¸€ç¶“éŠ·å•†èˆ‡ç¶“éŠ·å•†ä»£ç¢¼çš„ç›®æ¨™èˆ‡å¯¦éš›æª”æ¡ˆé…å°
- ç„¡æ³•é…å°å‰‡å‘ŠçŸ¥ç¼ºå°‘å“ªæ–¹
- é…å°å¾Œå‘¼å« compare_target_vs_actual(target_filename, actual_filename) é€²è¡ŒéŠ·å”®é”æ¨™åˆ†æ

åˆ¤æ–·ä¾æ“šèˆ‡å·¥å…·çµæœè«‹é€æ­¥èªªæ˜ã€‚

æ™‚é–“æ¬„ä½è™•ç†æµç¨‹ï¼š
1. è‹¥å•é¡Œæ¶‰åŠæ™‚é–“ç¯©é¸ï¼Œå…ˆç¢ºèªæ¬„ä½æ˜¯å¦ datetime å‹åˆ¥
2. é datetime åŸ·è¡Œ pd.to_datetime(æ¬„ä½, errors="coerce") ä¸¦è¦†å¯«
3. è½‰æ›å¾Œæª¢æŸ¥ NaT æ•¸é‡ï¼Œè¶…é 10% éœ€å›å ±è³‡æ–™å•é¡Œä¸¦åœæ­¢æ™‚é–“åˆ†æ
4. ç¢ºèªç‚º datetime å¾Œï¼Œæ–¹å¯ä½¿ç”¨ .dt å±¬æ€§æ“ä½œ

è³‡æ–™åˆ†ææµç¨‹ï¼š
1. ç¢ºèªæˆåŠŸè®€å– DataFrameï¼Œä¸”æ¬„ä½å­˜åœ¨ä¸”æ ¼å¼æ­£ç¢º
2. ç¼ºæ¬„ä½æˆ–è³‡æ–™ä¸è¶³ï¼Œå›è¦†éœ€è£œå……è³‡æ–™ï¼Œé¿å…åˆ†æå·¥å…·èª¿ç”¨
3. ç¢ºèªå¾Œæ‰å‘¼å« analyze_dataframe() é€²è¡Œåˆ†æ

åˆä½µèˆ‡é…å°è¦å‰‡ï¼š
1. ç¢ºèªå…©ä»½è³‡æ–™çš†å·²è¼‰å…¥ä¸”çµæ§‹æ˜ç¢ºï¼Œç¼ºä¸€å‰‡å›è¦†ã€Œè³‡æ–™ä¸è¶³ï¼Œè«‹æä¾›å®Œæ•´è³‡æ–™ã€
2. é…å°åªä¾ã€Œç¶“éŠ·å•†ä»£ç¢¼ã€èˆ‡ã€Œæ“šé»ä»£ç¢¼ã€å…©å€‹æ¬„ä½åšç²¾ç¢º merge  
3. åˆä½µå®Œæˆå¾Œï¼Œè‹¥å¯¦ç¸¾è¡¨ä¸­æœ‰ `ç¶“éŠ·å•†åç¨±` æˆ– `æ“šé»` æ¬„ï¼Œè«‹ç›´æ¥ä¸€ä½µè¼¸å‡ºï¼Œä¸ä½œç‚º join key
4. é…å°å®Œæˆé€²è¡ŒéŠ·å”®é”æ¨™å°æ¯”åˆ†æ

åˆ†æå·¥å…·ä½¿ç”¨åŸå‰‡ï¼š
- å„ªå…ˆä½¿ç”¨ pandas groupbyã€sortã€filter åšçµ±è¨ˆèˆ‡æ’åº
- éŠ·å”®é€²åº¦åˆ†æåªåŸºæ–¼ç¾æœ‰ç´€éŒ„
- å›å‚³é—œéµæ•¸æ“šèˆ‡ç°¡å–®è¡¨æ ¼ï¼Œé™„ä¸»è¦ç¯©é¸æ¢ä»¶èˆ‡ç¨‹å¼ç¢¼ï¼Œé¿å…éåº¦è§£é‡‹èˆ‡å¤§é‡åŸå§‹è³‡æ–™

åŸ·è¡Œåˆ†ææ­¥é©Ÿï¼š
1. list_files() ç¢ºèªå¯ç”¨æª”æ¡ˆ
2. åˆ†æå•é¡Œåˆ¤æ–·é—œéµå­—èˆ‡ç›®æ¨™æ¬„ä½
3. read_excel_head() é è¦½ç›¸é—œæª”æ¡ˆè¡¨é ­
4. è¼‰å…¥æœ€å¯èƒ½åŒ…å«è³‡æ–™çš„æª”æ¡ˆ
5. ç¢ºèªè³‡æ–™å®Œæ•´ä¸”æ ¼å¼æ­£ç¢º
6. é è¦½è¡¨é ­ï¼Œä»¥ç¶“éŠ·å•†é¡æ¬„ä½é…å°åˆä½µ
7. compare_target_vs_actual() åˆ†æéŠ·å”®é”æ¨™ç‹€æ³
8. å›å‚³çµæœèˆ‡é—œéµæ­¥é©Ÿç¨‹å¼ç¢¼ã€‚

å›ç­”è¦å‰‡ï¼š
- ä½¿ç”¨è€…å•çš„ç¶“éŠ·å•†åç¨±èˆ‡ç‡Ÿæ¥­æ‰€åç¨±å°±æ˜¯æ­¤æ¬¡è³‡æ–™æŸ¥è©¢çš„å”¯ä¸€æ¨™æº–ï¼Œä¸”çµæœå¿…é ˆåªåŒ…å«è©²ç¶“éŠ·å•†/ç‡Ÿæ¥­æ‰€çš„è³‡æ–™ã€‚
- è‹¥ç¶“éŠ·å•†åç¨±è®Šæ›´ï¼Œå¿…é ˆå®Œæ•´æ›´æ–°ä¸¦é‡ç½®è³‡æ–™ä¸Šä¸‹æ–‡ï¼Œä¸å¾—å¸¶å…¥ä¹‹å‰çš„ç¶“éŠ·å•†è³‡æ–™ã€‚
- è‹¥ä½¿ç”¨è€…è¼¸å…¥çš„æ˜¯ç¶“éŠ·å•†åç¨±èˆ‡ç‡Ÿæ¥­æ‰€åç¨±ï¼Œè«‹åƒç…§ä¸‹åˆ—å°æ‡‰è³‡è¨ŠæŸ¥æ‰¾å°æ‡‰çš„ä»£ç¢¼ï¼š{mapping_text}
- å¦‚æœä½¿ç”¨è€…å•ã€å®Œæ•´åˆ—å‡ºæ‰€æœ‰æ“šé»ã€ï¼Œè«‹å®Œæ•´è¼¸å‡ºç”¨æˆ¶æŒ‡å®šçš„ç¶“éŠ·å•†ä¸‹çš„æ‰€æœ‰æ“šé»è³‡æ–™ï¼ŒMarkdownè¡¨æ ¼æ ¼å¼ã€‚
- å¦‚æœå•ã€æŸæ“šé»é”æ¨™ç‹€æ³ã€ï¼Œåªå›ç­”è©²æ“šé»é”æ¨™ç‹€æ³ã€‚
- å¦‚æœå•ã€æŸç¶“éŠ·å•†é”æ¨™æ•¸é‡ã€ï¼Œè«‹å¾è©²ç¶“éŠ·å•†å®Œæ•´çš„æ‰€æœ‰æ“šé»è³‡æ–™ä¸­ï¼Œè¨ˆç®—ä¸¦å›è¦†é”æ¨™æ“šé»æ•¸ã€ç¸½æ“šé»æ•¸èˆ‡é”æ¨™ç‡ï¼Œæ‰€æœ‰æ“šé»å¿…é ˆå®Œæ•´åˆ—å‡ºï¼Œä¸”ä¸å¾—ç”¨æ¨¡ç³Šå­—çœ¼ï¼ˆå¦‚ï¼šå…¶ä»–æ“šé»ï¼‰æˆ–çœç•¥è™Ÿæ›¿ä»£ã€‚
- åœ¨è¼¸å‡º Markdown æ˜ç´°è¡¨å¾Œï¼Œæ‰€æœ‰ã€Œç¸½æ“šé»æ•¸ã€ã€ã€Œé”æ¨™æ“šé»æ•¸ã€èˆ‡ã€Œé”æ¨™ç‡ã€**å¿…é ˆ**å®Œå…¨ä¾ç…§ä¸‹åˆ—æ–¹å¼å¾è¡¨æ ¼æ–‡å­—ä¸­è§£æï¼Œ**ä¸å¾—**ä½¿ç”¨ä»»ä½•é¡å¤–è¨ˆç®—æˆ–éš±æ€§æ¨æ–·ï¼š
    1. **ç¸½æ“šé»æ•¸**ï¼šç›´æ¥æ•¸è¡¨æ ¼ä¸­æ¨™é¡Œåˆ—ä»¥ä¸‹ã€æ¯ä¸€è¡Œè³‡æ–™çš„è¡Œæ•¸ï¼ˆå³ Markdown åˆ—æ•¸æ¸›å»æ¨™é¡Œå’Œåˆ†éš”ç·šï¼‰ã€‚
    2. **é”æ¨™æ“šé»æ•¸**ï¼šé€è¡Œæª¢æŸ¥ã€Œé”æ¨™ã€æ¬„ï¼Œ**åªæœ‰**ç­‰æ–¼å­—ä¸² `True` çš„é‚£å¹¾è¡Œæ‰è¨ˆå…¥ï¼Œå…¶ä»–ä¸€å¾‹ä¸è¨ˆã€‚
    3. **é”æ¨™ç‡**ï¼šä»¥ã€Œé”æ¨™æ“šé»æ•¸ï¼ç¸½æ“šé»æ•¸ã€è¨ˆç®—ï¼Œä¸¦é¡¯ç¤ºç‚ºç™¾åˆ†æ¯”ï¼ˆä¿ç•™ä¸€ä½å°æ•¸ï¼‰ã€‚
    4. **åš´ç¦**é‡è¤‡æˆ–å¦è¡Œé‹ç®—ã€å°å…¥ DataFrame ç‰©ä»¶ã€æˆ–æ†‘å°è±¡å›æ¨ï¼›æ‰€æœ‰ summary æ•¸å­—éƒ½å¿…é ˆå’Œä¸Šæ–¹ Markdown è¡¨æ ¼çš„æ–‡å­—å®Œå…¨ä¸€è‡´ã€‚
- å…¶ä»–æƒ…æ³ï¼Œè«‹ä¾ä¸Šä¸‹æ–‡ç›¡é‡æº–ç¢ºå›è¦†ã€‚
- è‹¥ç„¡è¦æ±‚ï¼Œä¸é ˆé¡¯ç¤ºæ“šé»åç¨±ï¼Œå›ç­”æ™‚çš†ä»¥ä»£ç¢¼æä¾›ã€‚
"""

# å››ã€æµç¨‹æ§åˆ¶èˆ‡é¿å…ç„¡é™å¾ªç’°ï¼š
# 1. åœ¨äº’å‹•éç¨‹ä¸­ï¼Œé¿å…é‡è¤‡è©¢å•ç›¸åŒçš„å•é¡Œæˆ–è£œå……è³‡æ–™è¦æ±‚ã€‚
# 2. è‹¥ä½¿ç”¨è€…å¤šæ¬¡æœªæä¾›å®Œæ•´è³‡æ–™ï¼Œæ‡‰å›è¦†ã€Œå› è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•ç¹¼çºŒåˆ†æï¼Œè«‹æä¾›å®Œæ•´è³‡æ–™å¾Œå†è¡ŒæŸ¥è©¢ã€ä¸¦çµ‚æ­¢å¾ŒçºŒè‡ªå‹•è«‹æ±‚ã€‚
# 3. å›ç­”æ™‚è«‹æ¢ç†æ¸…æ™°ï¼Œæè¿°è™•ç†æ­¥é©Ÿèˆ‡ç¨‹å¼ç¢¼é‚è¼¯ï¼Œæ–¹ä¾¿ä½¿ç”¨è€…ç†è§£èˆ‡é©—è­‰ã€‚
# è«‹å‹™å¿…ç¢ºä¿å›ç­”æ™‚æ¢ç†æ¸…æ¥šï¼Œæœ‰ç†æœ‰æ“šï¼Œä¸¦é¿å…é‡è¤‡æˆ–ç„¡é™è¿´åœˆçš„äº’å‹•è¡Œç‚ºã€‚
# è«‹åœ¨åˆ†æéç¨‹ä¸­é¡¯ç¤ºå¦‚ä½•åˆ¤æ–·ã€Œç¶“éŠ·å•†ã€æ¬„ä½ï¼ˆåŒ…å«æ¨¡ç³Šæ¯”å°éç¨‹èˆ‡æ¬„ä½åç¨±ç¯©é¸ä¾æ“šï¼‰ï¼Œä»¥åˆ©ä½¿ç”¨è€…ç¢ºèªç³»çµ±ç†è§£æ­£ç¢ºã€‚
# è‹¥è³‡æ–™ä¸è¶³æˆ–å•é¡Œä¸æ˜ï¼Œè«‹å…ˆèªªæ˜ä¸¦è«‹æ±‚è£œå……ï¼Œé¿å…ç›²ç›®åˆ†æã€‚
# é€™æ¨£çš„å›ç­”æ¨¡å¼èƒ½ç¢ºä¿åˆ†ææº–ç¢ºã€äº’å‹•æœ‰æ•ˆã€‚

# ==================================== 5. Agent ====================================
prompt = ChatPromptTemplate.from_messages([
    ("system", system_message),
    ("human", "{input}"),
    ("ai", "{agent_scratchpad}")
    # LangChain çš„ Agent ç³»çµ±é æœŸä½ çš„ PromptTemplate è£¡æœƒæœ‰ä¸€å€‹å« agent_scratchpad çš„è®Šæ•¸ï¼Œ
    # ç”¨ä¾†è¨˜éŒ„ Agent æ­·å²çš„ intermediate stepsï¼ˆä¾‹å¦‚å·¥å…·èª¿ç”¨è¨˜éŒ„ã€æ€è€ƒéç¨‹ç­‰ï¼‰ã€‚
    # ç•¶ä½ å»ºç«‹è‡ªå®šç¾© Prompt ä¸”æ¼æ‰é€™å€‹è®Šæ•¸æ™‚ï¼ŒAgentExecutor å°±ç„¡æ³•æ­£å¸¸é‹ä½œã€‚
])

# å°‡å·¥å…·å‡½æ•¸è½‰æ›ç‚º OpenAI Functions æ ¼å¼
functions = [format_tool_to_openai_function(t) for t in tools]
# å»ºç«‹ Agent
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True
)

from langchain.callbacks import get_openai_callback


# æ¸¬è©¦ Agent
def query_agent(question: str) -> None:
    """å‘ Agent æå•ä¸¦é¡¯ç¤ºçµæœ"""
    print(f"å•é¡Œ: {question}")
    print("\næ­£åœ¨è™•ç†...\n")

    # ä½¿ç”¨ get_openai_callback ä¾†æ•ç²è©³ç´°çš„åŸ·è¡Œéç¨‹
    with get_openai_callback() as cb:
        response = agent_executor.invoke(
            {"input": question},
            return_intermediate_steps=True,  # ç¢ºä¿è¿”å›ä¸­é–“æ­¥é©Ÿ
            include_run_info=True  # åŒ…å«é‹è¡Œä¿¡æ¯
        )

    print("\nå›ç­”:")
    print(response["output"])

    # é¡¯ç¤ºåŸ·è¡Œæ—¥èªŒ
    if "intermediate_steps" in response:
        print("\nåŸ·è¡Œéç¨‹:")
        for i, step in enumerate(response["intermediate_steps"]):
            tool = step[0].tool
            tool_input = step[0].tool_input
            tool_output = step[1]

            print(f"\næ­¥é©Ÿ {i + 1}:")
            print(f"å·¥å…·: `{tool}`")
            print(f"è¼¸å…¥: `{tool_input}`")
            print(f"è¼¸å‡º: {tool_output}")
            print("-" * 50)

    # å¦‚æœä½¿ç”¨äº† analyze_dataframeï¼Œå˜—è©¦æ•ç²å…¶å…§éƒ¨ Pandas æ“ä½œ
    if any(step[0].tool == "analyze_dataframe" for step in response.get("intermediate_steps", [])):
        print("\nPandas æ“ä½œè©³æƒ…å°‡åœ¨åŸ·è¡Œéç¨‹ä¸­é¡¯ç¤ºã€‚")

    print(f"\nåŸ·è¡Œçµ±è¨ˆ:")
    print(f"ç¸½ä»¤ç‰Œä½¿ç”¨: {cb.total_tokens} ä»¤ç‰Œ")
    print(f"ç¸½èŠ±è²»: ${cb.total_cost:.6f}")
    print(f"æˆåŠŸèª¿ç”¨æ¬¡æ•¸: {cb.successful_requests}")

    return response

# ==================================== ä½¿ç”¨ç¯„ä¾‹  ====================================
if __name__ == "__main__":
    user_input = "å“ªå€‹è»Šæ¬¾è²©å”®å¾—æœ€å°‘ï¼Ÿ"

    with get_openai_callback() as cb:
        response = agent_executor.invoke({"input": user_input})

    print("ğŸ§¾ åˆ†æçµæœï¼š")
    print(response["output"])